---
title: "Computational Musicology Portofolio"
author: "Kalle"
date: "2/10/2021"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    social: menu
    storyboard: true
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(protoclust)
library(heatmaply)
library(cowplot)
library(rbokeh)
library(plotly)
library(spotifyr)
library(compmus)
```

{data-width=600}
-----------------------------------------------------------------------

### Kwaito. A computational analysis of the South African (sub)genre {data-commentary-width=500}

<div style="max-width:600px; margin: 0 auto;">
<h2>Kwaito</h2>


This computational analysis explores the musical features of "Kwaito", a South African music genre that emerged in Johannesburg during the 1990s. Kwaito is labeled by some as 'South African Hip-Hop'. Others say it's more like slowed down House music. Some people argue it is "just Kwaito, and you can't compare it to anything". It's exact roots have remained a guess for most people still today. In this musicological analysis I'll try to reveal some of Kwaito's mystery, by looking at the Spotify features of typical Kwaito music. These will then be compared to other related styles of music that came up around that time.

In the following tabs you’ll find a data analysis of Spotify’s track level features such as tempo, valence and danceability. We’ll also inspect the chroma features of classic Kwaito songs and touch upon the loudness and timbre features and look in to the structural segments, beats and rhythm. Combining different forms of musical analysis we hope to gain insights in the characteristics of the Kwaito sound. Through a broad range of analytic scopes we hope to discover to what and to what extend Kwaito links to other musical styles, like the two already mentioned. A third objective for this project is to use the process to evaluate and test the possibilities and limitations of the Spotify API.

### A brief history of Kwaito {data-commentary-width=500}

<div style="max-width:600px; margin: 0 auto;">
<h2>A brief history</h2>


The sound of Kwaito developed in the late 1980’s in the townships of South Africa and came to be the soundtrack for a youth culture movement. With it's peak in the 90's the Kwaito sound also started to spread globally. From then on, collectors and diggers across the globe started discovering the genre and finding its endless source of inspiration. As they do, Kwaito is still fuelling dance floors around the world to this day. The broad and varied form of Kwaito makes it very hard to encapsulate completely as a genre. Kwaito fuses elements of traditional South African music with other genres throughout the African diaspora.

Kwaito was originally heard as simply slowed-down house music. The repetitive four on the floor beat and swung high hat patterns are some of the characteristics that closely link to the classic structure of house music. What's for sure is that it holds the power to make dance floors shake. In general the sound is known for it's danceability, making it's way to the international clubs and festival circuit. Others recognise a strong hip-hop influence. The late ‘80s the township slang of South Africa has formed the backbone of Kwaito. Reflecting on social changes and cavernous cracks in South-African society, Kwaito is labeled by some as 'South African Hip-Hop'. It's influence becomes evident as a number of Kwaito songs have clear references to American hip-hop forms, paying homage to artists like Snoop Dogg and Grandmaster Flash.

Defining Kwaito as a musical genre seems to be a struggling process still today, making it a fascinating and compelling subject of research. In this portfolio we'll explore the musical boundaries of the South African (sub)genre and try to reveal it's relation to more classical styles like Hip-Hop and House. Finally, in our search of the true Kwaito sound, we'll take the opportunity to test and evaluate the possibilities and limitations of the Spotify API.

### Working with Spotify API. A variety of track-level features {data-commentary-width=500}

<div style="max-width:600px; margin: 0 auto;">
<h2>Working with Spotify API</h2>


The Spotify API offers a variety of track-level features for all music on Spotify. Because of the enormous amount of data that is accessible through the API, it is important to consider only the most relevant options for the study. The plot shows a number of track-level features from the three playlists, each representing their own musical style. Respectively the genres Hip-Hop, Kwaito and House are compared. All three playlists contain a selection of 50 tracks that are supposed to be representative for the genre. Of course we should keep in mind that a musical genre is a complex social construct that has shifting boundaries. For this study I chose playlists that have got a reasonable 'following' on Spotify. This choice is deliberate, building on Sociological theories that argue musical genre in itself, is a social construct which exists by the legitimacy of people, and grows more solid as it gets more followers and eventually consolidates over time. The playlists consists of a relatively clearly demarcated selection of tracks that positions itself within the boundaries of the genre. 

For this study I chose the following track-level features: 

**Speechiness**: this detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.

**Tempo**. This is the overall estimated tempo of a track in beats per minute (bpm). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
The **speechiness** feature covers the **x-axes** and the **tempo** feature the **y-axes**. The 'tempo' feature, seemed to be the best option in my case. Tempo in general is a strong feature because it carries a high amount of deterministic value. The tempo of a song has proved to be an essential factor in the way people give meaning to music and how they perceive it, consciously as well as unconsciously. Tempo can also be seen as representative because it can easily be used as tool to distinguish a genre from others in an easy, quantifiable manner that is widely known and used.

**Danceability**. The value for danceability is shown by lightness of color, lighter being valued as more danceable. This seemed like a relevant feature, since dance culture is closely linked to the cultural norms of the three genres we are focusing on.


### Hip-Hop, House or Kwaito? Comparing the track-level features {data-commentary-width=500}

```{r}

KWAITO_plot <- get_playlist_audio_features("", "48lmHgufe6tzfltv1MtA00") %>% slice_head(n = 50)
HOUSE_plot <- get_playlist_audio_features("", "37i9dQZF1DWTU3Zl0elDUa") %>% slice_head(n = 50)
HIPHOP_plot <- get_playlist_audio_features("", "2MOqzRnIikBt5jiruMRv0r") %>% slice_head(n = 50)
DRIE_GENRES <-
  KWAITO_plot %>%
  mutate(country = "KWAITO") %>%
  bind_rows(HOUSE_plot %>% mutate(country = "CLASSIC HOUSE MUSIC")) %>%
  bind_rows(HIPHOP_plot %>% mutate(country = "CLASSIC HIP HOP")) %>%
  mutate(
    country = fct_relevel(country, "CLASSIC HIP HOP", "KWAITO", "CLASSIC HOUSE MUSIC")
  )

DE_GROTE_VERGELIJKING <-
  DRIE_GENRES %>%
  ggplot(                          # Set up the plot.
    aes(
      x = speechiness,
      y = tempo,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point() +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~country) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 0.50),
    breaks = c(0, 0.10, 0.20, 0.30, 0.40, 0.50),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(80, 140),
    breaks = c(80, 90, 100, 110, 120, 130, 140),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "D",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
    guide = "none"
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
    guide = "none"                 # Remove the legend for size.
  ) +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "SPEECHINESS",
    y = "TEMPO"
  )

ggplotly(DE_GROTE_VERGELIJKING)
```
*** 

Looking at the distribution for the y-axes we see a clear tempo difference between the three genres. The selection house tracks ranges anywhere between 120-130 bpm, which is typical for it's style. The selection Kwaito has a little bit of a broader range, but the greater part is positioned in the range of 100-110. This seems in line with the associations we discussed earlier, in which Kwaito was described as "slowed down house music". 

When we look at speechiness we are also seeing some variation between the genres. Hip hop tracks are valued most speechiness, next Kwaito, and coming in last House. All of the tracks from every genre are valued <0.30. According to the Spotify description of speechiness you could therefore only say 'they are likely to represent music and other non-speech-like tracks'. I was expecting to see more speechiness in the kwaito tracks, as Kwaito is often being described as "the South African Hip-Hop" and it is especially known for spoken lyrics and talk overs. Kwaito was eminently known as a way for people to get their voice heard, like a catalyst used for political purposes. This even got to the point Kwaito got banned for it by the South African government. However the speechiness feature doesn't really give us a real clue about these aspects of Kwaito.

As for the danceability feature you can see there is far more value in Hip Hop and Kwaito tracks than in House tracks. This is remarkable because House music has a strong club culture. House as a genre I would associate most with dancing, as House tracks are usually created for this purpose. Usually in my experience if there is House music played, regardless of where or the occasion, there is dancing. When Hip-Hop is played for example, again based on my own experience, this is not as often the case. Spotify describes danceability as 'how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity'. While there is something to say for the fact that dynamic music with lots of action and changes is making people move, it could be they underestimate the power of stable rythm repetitiveness that makes people dance as if in a trance.

### About tempo! {data-width=600}

```{r}
get_tidy_audio_analysis('4PyhlL1vrSvPlhYLdphszr') %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo') +
    theme_classic()
```

***

Tempograms provide insight in how tempo evolves during a musical piece. The x-axis contains the song duration and the y-axis contains the measured tempo in beats per minutes. Brighter areas indicate larger amounts of energy. 

I was curious if Kwaito songs ar produced live or that it is fully digital and therefore synchronized to a fixed grid. 

The tempogram is of a Kwaito song from the playlist. We are seeing a steady line that doesn't really change over time. This would suggest that the tempo remains stable over the course of the song. This indicates Kwaito is digitally produced rather than recorded with live instruments. To be sure I did a tempogram for a few other tracks and this seemed the case for most. 

The tempo in bpm in this tempogram seems to be around 440. This is obviously very unlikely. Sometimes the tempo of a song is multiplied. With downtempo songs like soul or slow Hip-Hop this is often the case. It shows why beat analysis could be problematic. A fix for this might be to set a range in bpm of which you are sure the track must be in, for example between 50-200 beats per minutes. I have seen programs, mostly DJ software, that actually are pretty good at bpm estimation. I guess this is one of the things that can therefore be improved in the Spotify API.

### Dynamic Time Warping, B.I.G vs Kwaito version

```{r}
SandyB <-
  get_tidy_audio_analysis("5WltyA542cYUVj8NCgOjIR") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

BIG <-
  get_tidy_audio_analysis("63BcfK6YAzJYeISaTPr6IO") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

compmus_long_distance(
  SandyB %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  BIG %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "manhattan"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "BIG", y = "Sandy B") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```

***

One of the things you can do with Spotify API is visual Dynamic Time Warping. With Dynamic Time Warping you can compare the pitches of two tracks in a single visualization. You can for example line up a cover version of a song with the original track or even another cover version of the song, and see how they differ from each other. 

To see if Kwaito differs a lot from Hip-Hop we are doing a test case with a classic Hip-Hop song: B.I.G.- Big Poppa, and a Kwaito version that uses the same melody: Sandy B - Student Night.

A diagonal pattern would denote similarities between the two tracks. This is not observed, which implies significant differences. This could suggest that the Kwaito cover is a lot different from a Hip-Hop song. However, we can't really generalize this to say all Kwaito songs differ a lot from Hip-Hop tracks. 

The fact that two tracks using the same melody sound so different you could see as an indication of their independence to each other. On the other hand, you could say the use of a classic Hip-Hop sample by a Kwaito artist is an indication that there is a linkage between the genres. 


### Self-Similarity: The chroma- and timbre features of a 'Classic" **Kwaito** track

```{r}
kwaito_typisch <-
  get_tidy_audio_analysis("7jCN8HmG6EaUzQKq9uHEH2") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  kwaito_typisch %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  kwaito_typisch %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***

To get some insights in the similarities and differences between Hip Hop, Kwaito and House we analyze two self-similarity matrices for a single track from each genre individually. Both the chroma and timbre matrices could be useful to understand the structure of a song. We give two plots for each track: the first being chroma, and the second timbre. By denoting patterns of similarities that reoccur these plots show us the structure of a track. For example, diagonal lines and a checkerboard pattern show similarity and repetition. The diagonal line is expected because the track was plotted against itself. 

First: a track from the Kwaito playlist

The timbre similarity matrix shows us a faint checkerboard pattern, which implies some form of repetition in the track. The matrix was able to pick up some of the presentations of the chorus. It looks like the Kwaito track follows some sort of pattern where the song starts with some sort of chorus, followed by a verse, chorus, verse etc.

The duration of the Kwaito song is 350 seconds, which is pretty long but not uncommon for Kwaito music. Compared to most Hip-Hop tracks however this is relatively long.

### Self-Similarity: The chroma- and timbre features of a 'Classic' **House** track

```{r}
house_typisch <-
  get_tidy_audio_analysis("4PyhlL1vrSvPlhYLdphszr") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  house_typisch %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  house_typisch %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***

Second: a track from the House playlist. 

The House track 'Big Fun', from Inner City is a classic. Both plots show a clear cross in the middle. Around the 100 second mark there is a significant chroma and timbre difference. This is because after 100 sec a rather abrubt break comes in. The synthesizers in the track drop out completely, and after about 20 second the synths come back in and the party can start again. This break can be seen in both the chroma- and the timbre matrix. In contrast to the Kwaito Matrix we saw earlier we don't really see a chorus or verse in this one. 

The duration of the House track is just over 200 seconds, which is not too long for a House track. Often tracks in this genre have a repetitive intro and outro which is used to mix the track with another. I expect there to be a longer version as well that is played more in clubs.


### Self-Similarity: The chroma- and timbre features of a 'Classic" **Hip Hop** track

```{r}
hip_typisch <-
  get_tidy_audio_analysis("2Dts49OexROL2KvCK2sokf") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  hip_typisch %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  hip_typisch %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***

Third: a track from the Hip-Hop playlist. 

We are looking at the classic Hip-Hop track 'What's the Difference' from Dr. Dre. This matrices show the most clear checkerboard we have seen until now. The matrices are able to pick up a pattern including the presentations of the chorus. It seems Hip-Hop song follows a similar pattern as the Kwaito song, with some sort of chorus, followed by a verse, chorus etc.

The duration of 'What's the Difference' is nearly 250 seconds. This is pretty long for Hip-Hop, but not uncommon. Hip-Hop tracks from around the 'Dr Dre era' also were a bit longer compared to Hip-Hop nowadays.

### Confusion Matrix of Hip-Hop, House and Kwaito

```{r}
library(tidyverse)
library(tidymodels)
library(spotifyr)
library(ggdendro)
library(heatmaply)
library(compmus)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}

HIPHOP <- 
  get_playlist_audio_features("spotify", "2MOqzRnIikBt5jiruMRv0r")
HOUSE <- get_playlist_audio_features("spotify", "37i9dQZF1DWTU3Zl0elDUa")
KWAITO <- get_playlist_audio_features("spotify", "48lmHgufe6tzfltv1MtA00")
all_three_genres <-
  bind_rows(
    HIPHOP %>% mutate(playlist = "HIP HOP") %>% slice_head(n = 50),
    HOUSE %>% mutate(playlist = "CLASSIC HOUSE") %>% slice_head(n = 50),
    KWAITO %>% mutate(playlist = "SA KWAITO") %>% slice_head(n = 50)
  )

all_three_genres_features <-
all_three_genres %>%
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))

all_three_genres_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = all_three_genres_features,
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

all_three_genres_cv <- all_three_genres_features %>% vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
all_three_genres_knn <- 
  workflow() %>% 
  add_recipe(all_three_genres_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    all_three_genres_cv, 
    control = control_resamples(save_pred = TRUE)
  )

all_three_genres_knn %>% get_conf_mat()

all_three_genres_knn %>% get_conf_mat() %>% autoplot(type = "mosaic")

all_three_genres_knn %>% get_conf_mat() %>% autoplot(type = "heatmap")

all_three_genres_knn %>% get_pr()

tree_model <-
  decision_tree() %>%
  set_mode("classification") %>% 
  set_engine("C5.0")
all_three_genres_tree <- 
  workflow() %>% 
  add_recipe(all_three_genres_recipe) %>% 
  add_model(tree_model) %>% 
  fit_resamples(
    all_three_genres_cv, 
    control = control_resamples(save_pred = TRUE)
  )

all_three_genres_tree %>% get_pr()

```

*** 

This classification algorithm is going to make predictions after the training. For me to be able to make a difference between Kwaito, House and Hip-Hop, I need to know for every track what it actually is. The classification algorithm is going to make a prediction for a new track to whether the track is Kwaito, House and Hip-Hop. Then, the question remains: how good is my classifier?
To get the most accurate picture of what my classifier is doing I actually want to look at these per class. So if I want to compare Kwaito, House and Hip-Hop, I want to know for Hip-Hop what is my recall and what is my precision, for House what is my recall what is my precision etc. What do these mean? Well there are all based on the confusion matrix. 

This matrix is basically breaking things down into 4 possibilities. So you look at the so called ground truth class, what is it really? The difference between precision and recall is which mistakes you look at. These numbers are not the same. If you have very good recall, often the precision is not so great. Often if you have good precision, recall not so good. So, precision is when you look only at just the predictions, so of everything my classifier said was Hip Hop, how many of those really are Hip Hop? Recall is when you flip precision around. If I look at everything in my corpus that actually is Hip Hop, and how many of those did the classifier find?

Now, we evaluate. Looking at the table we can see the classifier did a pretty good job. It was actually very accurate in predicting the songs genre. This would suggest all three genres, looking at their Spotify features, are not so similar after all. 

### Conclusion {data-commentary-width=500}

Defining Kwaito as a genre has not been proven easy. It was however a very interesting case study, showing all the ways of analyzing music and the variety of interpretations one can have of a musical genre. In this portfolio we explored the musical boundaries of Kwaito and hoped to discover more about it's relation to other genres like Hip-Hop and House. Where Kwaito is labeled by some as 'South African Hip-Hop' and by others as a 'slowed down version of House', I would agree with people saying it is "just Kwaito, and you can't compare it to anything!". 


The Spotify API provided us with varies ways to show Kwaito's uniqueness. Looking at the tempo feature we could see Kwaito has it's own range in BPM, significantly different from House and Hip-Hop. The comparison of the Kwaito version of a classic Hip-Hop track also showed us the originality of Kwaito. Most convincing to me however was the classification tool. We could see the classifier did a very good job in predicting the songs genre. This would suggest all three genres, looking at their Spotify features, are unique in it's kind. 


Finally, we tried to use this portfolio for the evaluation of the Spotify API. 
We took the opportunity to test and evaluate the possibilities and limitations of the Spotify API. First of all I want to say it is a very fun tool to use and it gives endless possibilities. While this is great, it could also be a pitfall. Sometimes you can get caught up in a single track-feature with the risk of getting in tunnel vision, making causal relationships that might not be there. If you want to compare whole genres of music it is therefore important to keep an overview. Sometimes taking a step back and thinking is this relevant, or asking yourself if your assumptions are correct can help to keep your head strait. A background in statistics would be very helpful in this all I guess. 


But in the end, Spotify making this API available for everyone without any certifications is also what makes it so beautiful. In a world where more and more music is based on one platform such as Spotify I think this is a blessing. Making this data available for everyone provides endless possibilities to study the music we love.
